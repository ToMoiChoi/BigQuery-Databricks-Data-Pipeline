# ğŸ“Š BigQuery â†’ Databricks Data Pipeline



Pipeline Python tá»± Ä‘á»™ng trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« **Google BigQuery** vÃ  upload lÃªn **Databricks** (Delta Table).



## ğŸ“ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚  query  â”‚                  â”‚  INSERT  â”‚                  â”‚
â”‚   Google     â”‚â”€â”€â”€â”€â”€â”€â”€â†’ â”‚  Python Pipeline â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚   Databricks     â”‚
â”‚   BigQuery   â”‚  REST   â”‚  (pandas + SQL)  â”‚  SQL     â”‚   Delta Table    â”‚
â”‚              â”‚  API    â”‚                  â”‚  Conn.   â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Dataset:                  Modules:                    Catalog:
   datalize                  - bigquery_extract.py       datalize.view.*
   (25 tables)               - databricks_upload.py
                             - config.py
```

## ğŸ“ Cáº¥u trÃºc Project

```
â”œâ”€â”€ config.py               # Load & validate cáº¥u hÃ¬nh tá»« .env
â”œâ”€â”€ bigquery_extract.py     # Module trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« BigQuery
â”œâ”€â”€ databricks_upload.py    # Module upload dá»¯ liá»‡u lÃªn Databricks
â”œâ”€â”€ main.py                 # Pipeline CLI (cháº¡y tá»«ng table)
â”œâ”€â”€ run_all.py              # Pipeline batch (cháº¡y Táº¤T Cáº¢ tables)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                    # Biáº¿n mÃ´i trÆ°á»ng (credentials) - KHÃ”NG commit!
â”œâ”€â”€ .env.example            # Template biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ gcp-key.json            # Google Cloud Service Account key - KHÃ”NG commit!
â””â”€â”€ README.md               # File nÃ y
```

## ğŸš€ CÃ i Ä‘áº·t

### 1. Táº¡o virtual environment
```bash
python -m venv .venv
.venv\Scripts\Activate.ps1   # Windows PowerShell
# hoáº·c
source .venv/bin/activate     # Linux/Mac
```

### 2. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 3. Cáº¥u hÃ¬nh credentials

Copy `.env.example` â†’ `.env` vÃ  Ä‘iá»n thÃ´ng tin:

```bash
cp .env.example .env
```

| Biáº¿n | MÃ´ táº£ | VÃ­ dá»¥ |
|------|--------|-------|
| `BIGQUERY_PROJECT_ID` | GCP Project ID | `tensile-cogency-408304` |
| `BIGQUERY_CREDENTIALS_PATH` | ÄÆ°á»ng dáº«n tá»›i Service Account JSON key | `gcp-key.json` |
| `BIGQUERY_DATASET` | Dataset trÃªn BigQuery | `datalize` |
| `DATABRICKS_HOST` | URL workspace Databricks (pháº£i cÃ³ `https://`) | `https://dbc-xxx.cloud.databricks.com` |
| `DATABRICKS_TOKEN` | Personal Access Token | `dapi0eb9d5c...` |
| `DATABRICKS_HTTP_PATH` | HTTP Path cá»§a SQL Warehouse | `/sql/1.0/warehouses/xxx` |
| `DATABRICKS_CATALOG` | Catalog trÃªn Databricks | `datalize` |
| `DATABRICKS_SCHEMA` | Schema trÃªn Databricks | `view` |

> âš ï¸ **LÆ°u Ã½**: KHÃ”NG commit `.env` vÃ  `gcp-key.json` lÃªn Git. ThÃªm vÃ o `.gitignore`.

## ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Cháº¡y toÃ n bá»™ dataset (run_all.py)

Láº¥y **Táº¤T Cáº¢** tables tá»« BigQuery dataset vÃ  upload lÃªn Databricks:

```bash
python run_all.py
```

**Output máº«u:**
```
2026-02-23 11:00:00 | INFO | PIPELINE: BigQuery â†’ Databricks (ALL TABLES)
2026-02-23 11:00:01 | INFO | Found 25 tables: ['Attendance', 'Department1', ...]
2026-02-23 11:00:02 | INFO | ğŸ“¦ [1/25] Processing table: Attendance
2026-02-23 11:00:03 | INFO |    âœ… Extracted 3039 rows, 19 columns
2026-02-23 11:00:10 | INFO |    âœ… Uploaded successfully!
...
2026-02-23 11:05:00 | INFO | ğŸ PIPELINE COMPLETED in 300.0s
2026-02-23 11:05:00 | INFO |    âœ… Success: 25/25 tables
```

### Cháº¡y tá»«ng table (main.py)

```bash
# Upload 1 báº£ng lÃªn DBFS dáº¡ng Parquet
python main.py --table Attendance --method dbfs

# Upload 1 báº£ng vÃ o Delta Table qua SQL INSERT
python main.py --table Attendance --method sql_insert --target attendance_table

# Cháº¡y custom SQL query
python main.py --query "SELECT * FROM datalize.Attendance WHERE employee = 'Nguyen Van A'" \
               --method sql_insert --target filtered_data

# Giá»›i háº¡n sá»‘ dÃ²ng
python main.py --table Attendance --method sql_insert --limit 100 --target attendance_sample

# Liá»‡t kÃª táº¥t cáº£ tables trong BigQuery dataset
python main.py --list-tables
```

## ğŸ”§ Chi tiáº¿t cÃ¡c Module

### `config.py` â€” Quáº£n lÃ½ cáº¥u hÃ¬nh

| Class | MÃ´ táº£ |
|-------|--------|
| `BigQueryConfig` | Load & validate BigQuery credentials tá»« `.env` |
| `DatabricksConfig` | Load & validate Databricks credentials tá»« `.env` |

Cáº£ 2 class Ä‘á»u cÃ³ method `validate()` kiá»ƒm tra Ä‘áº§y Ä‘á»§ config trÆ°á»›c khi káº¿t ná»‘i.

---

### `bigquery_extract.py` â€” TrÃ­ch xuáº¥t dá»¯ liá»‡u

| Method | MÃ´ táº£ | Return |
|--------|--------|--------|
| `extract_by_query(query)` | Cháº¡y SQL query tÃ¹y chá»‰nh | `pandas.DataFrame` |
| `extract_table(table_name, limit)` | Láº¥y toÃ n bá»™ báº£ng | `pandas.DataFrame` |
| `list_tables(dataset)` | Liá»‡t kÃª báº£ng trong dataset | `List[str]` |
| `get_table_schema(table_name)` | Láº¥y schema (tÃªn cá»™t, kiá»ƒu dá»¯ liá»‡u) | `List[dict]` |

**CÃ¡ch sá»­ dá»¥ng:**
```python
from bigquery_extract import BigQueryExtractor

extractor = BigQueryExtractor()

# Láº¥y toÃ n bá»™ báº£ng
df = extractor.extract_table("Attendance")

# Cháº¡y query tÃ¹y chá»‰nh
df = extractor.extract_by_query("SELECT * FROM datalize.Attendance LIMIT 100")

# Liá»‡t kÃª báº£ng
tables = extractor.list_tables()  # â†’ ['Attendance', 'Department1', ...]
```

---

### `databricks_upload.py` â€” Upload dá»¯ liá»‡u

| Method | MÃ´ táº£ | Khi nÃ o dÃ¹ng |
|--------|--------|--------------|
| `upload_to_dbfs(df, path, format)` | Upload file Parquet/CSV lÃªn DBFS | LÆ°u trá»¯ file, dataset lá»›n |
| `upload_to_delta_table(df, table, mode)` | Táº¡o Delta Table tá»« staged Parquet | Cáº§n DBFS access |
| `write_with_sql_connector(df, table, mode)` | INSERT tá»«ng batch qua SQL | **Recommend** â€” khÃ´ng cáº§n DBFS |

> ğŸ’¡ **Khuyáº¿n nghá»‹**: DÃ¹ng `write_with_sql_connector()` vÃ¬ khÃ´ng yÃªu cáº§u quyá»n DBFS.

**CÃ¡ch sá»­ dá»¥ng:**
```python
from databricks_upload import DatabricksUploader

uploader = DatabricksUploader()

# Upload qua SQL INSERT (khuyáº¿n nghá»‹)
uploader.write_with_sql_connector(df, "my_table", mode="overwrite")

# Upload file lÃªn DBFS (cáº§n quyá»n DBFS)
uploader.upload_to_dbfs(df, "/FileStore/data/my_table.parquet")
```

**TÃ­nh nÄƒng:**
- âœ… Tá»± Ä‘á»™ng sanitize tÃªn cá»™t (loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t cho Delta Lake)
- âœ… Xá»­ lÃ½ datetime, boolean, NULL values Ä‘Ãºng cÃ¡ch
- âœ… Batch INSERT (1000 rows/batch) â€” trÃ¡nh quÃ¡ táº£i SQL
- âœ… Streaming upload cho file lá»›n (> 1MB) trÃªn DBFS
- âœ… Há»— trá»£ mode `overwrite` vÃ  `append`

---

### `run_all.py` â€” Pipeline batch

Tá»± Ä‘á»™ng:
1. Káº¿t ná»‘i BigQuery â†’ liá»‡t kÃª táº¥t cáº£ tables
2. Extract tá»«ng table â†’ `pandas.DataFrame`
3. Sanitize tÃªn cá»™t â†’ loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
4. Upload lÃªn Databricks qua SQL INSERT
5. Log káº¿t quáº£ tá»•ng há»£p (success/error count)

---

### `main.py` â€” Pipeline CLI

Há»— trá»£ arguments:

| Argument | MÃ´ táº£ | Default |
|----------|--------|---------|
| `--table, -t` | TÃªn báº£ng BigQuery | â€” |
| `--query, -q` | SQL query tÃ¹y chá»‰nh | â€” |
| `--method, -m` | PhÆ°Æ¡ng thá»©c upload: `dbfs`, `delta`, `sql_insert` | `dbfs` |
| `--target` | TÃªn báº£ng Ä‘Ã­ch trÃªn Databricks | = source table |
| `--mode` | `overwrite` hoáº·c `append` | `overwrite` |
| `--format` | `parquet` hoáº·c `csv` (cho DBFS) | `parquet` |
| `--limit, -l` | Giá»›i háº¡n sá»‘ dÃ²ng | â€” |
| `--list-tables` | Liá»‡t kÃª táº¥t cáº£ tables | â€” |

## ğŸ“‹ Dá»¯ liá»‡u BigQuery Dataset `datalize`

Pipeline Ä‘Ã£ phÃ¡t hiá»‡n **25 tables** trong dataset:

| # | Table Name | MÃ´ táº£ |
|---|-----------|--------|
| 1 | `Attendance` | Dá»¯ liá»‡u cháº¥m cÃ´ng |
| 2 | `Department1` | ThÃ´ng tin phÃ²ng ban |
| 3 | `Employee_infor` | ThÃ´ng tin nhÃ¢n viÃªn |
| 4 | `Employee_infor1` | ThÃ´ng tin nhÃ¢n viÃªn (báº£n 2) |
| 5 | `Group` | NhÃ³m |
| 6 | `Group_v2` | NhÃ³m (phiÃªn báº£n 2) |
| 7 | `Holiday` | NgÃ y nghá»‰ lá»… |
| 8 | `Shift_ok` | Ca lÃ m viá»‡c |
| 9 | `attendance_results_chancekim` | Káº¿t quáº£ cháº¥m cÃ´ng |
| 10 | `dahahi_devicesList` | Danh sÃ¡ch thiáº¿t bá»‹ |
| 11 | `dahahi_employeesList` | Danh sÃ¡ch nhÃ¢n viÃªn |
| 12 | `etl_control` | ETL control metadata |
| 13 | `hubspot_companies` | Dá»¯ liá»‡u cÃ´ng ty tá»« HubSpot |
| 14 | `hubspot_contacts` | Dá»¯ liá»‡u liÃªn há»‡ tá»« HubSpot |
| 15 | `lark_studentsInfo` | ThÃ´ng tin há»c sinh tá»« Lark |
| 16 | `lark_thongtingiaovien` | ThÃ´ng tin giÃ¡o viÃªn tá»« Lark |
| 17 | `lark_thongtinhocvien` | ThÃ´ng tin há»c viÃªn tá»« Lark |
| 18 | `lark_thongtinlophoc` | ThÃ´ng tin lá»›p há»c tá»« Lark |
| 19 | `larktask` | Tasks tá»« Lark |
| 20 | `larktasktest` | Tasks test tá»« Lark |
| 21 | `shopify_orders` | ÄÆ¡n hÃ ng Shopify |
| 22 | `shopify_orders_raw` | ÄÆ¡n hÃ ng Shopify (raw) |
| 23 | `test` | Báº£ng test |
| 24 | `test2` | Báº£ng test 2 |
| 25 | `vw_shopify_orders_latest` | View Ä‘Æ¡n hÃ ng Shopify má»›i nháº¥t |

## ğŸ”’ Báº£o máº­t

ThÃªm vÃ o `.gitignore`:
```
.env
gcp-key.json
.venv/
__pycache__/
*.pyc
```

## âš ï¸ Troubleshooting

| Lá»—i | NguyÃªn nhÃ¢n | Giáº£i phÃ¡p |
|-----|-------------|-----------|
| `403 Forbidden (DBFS)` | Token khÃ´ng cÃ³ quyá»n DBFS | DÃ¹ng `sql_insert` method thay vÃ¬ `dbfs`/`delta` |
| `PARSE_SYNTAX_ERROR` | GiÃ¡ trá»‹ datetime khÃ´ng Ä‘Æ°á»£c quote | ÄÃ£ fix â€” update `databricks_upload.py` má»›i nháº¥t |
| `DELTA_INVALID_CHARACTERS` | TÃªn cá»™t chá»©a kÃ½ tá»± Ä‘áº·c biá»‡t | ÄÃ£ fix â€” tá»± Ä‘á»™ng sanitize trong `run_all.py` |
| `PAT token error` | Token háº¿t háº¡n | Táº¡o má»›i token trÃªn Databricks â†’ Settings â†’ Developer |
| `BigQuery Storage warning` | Thiáº¿u module storage | `pip install google-cloud-bigquery-storage` |

## ğŸ“ Dependencies

```
google-cloud-bigquery          # BigQuery client
google-cloud-bigquery-storage  # BigQuery Storage API (tÄƒng tá»‘c)
pandas                         # DataFrame processing
pyarrow                        # Parquet support
db-dtypes                      # BigQuery data types
databricks-sql-connector       # Databricks SQL Connector
databricks-sdk                 # Databricks SDK
python-dotenv                  # Load .env file
requests                       # HTTP requests (DBFS API)
```
#   B i g Q u e r y - D a t a b r i c k s - D a t a - P i p e l i n e 
 
 

