"""
Script láº¥y Táº¤T Cáº¢ tables tá»« BigQuery dataset vÃ  upload lÃªn Databricks.

Usage:
    python run_all.py
"""

import logging
import sys
from datetime import datetime
from bigquery_extract import BigQueryExtractor
from databricks_upload import DatabricksUploader

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("run_all")


def main():
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("PIPELINE: BigQuery â†’ Databricks (ALL TABLES)")
    logger.info("=" * 60)

    # â”€â”€ Step 1: Connect to BigQuery & list all tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ğŸ“¥ Connecting to BigQuery...")
    extractor = BigQueryExtractor()

    logger.info(f"ğŸ“‹ Listing tables in dataset: {extractor.dataset}")
    tables = extractor.list_tables()

    if not tables:
        logger.warning("âš ï¸  No tables found in dataset!")
        sys.exit(0)

    logger.info(f"Found {len(tables)} tables: {tables}")

    # â”€â”€ Step 2: Connect to Databricks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ğŸ“¤ Connecting to Databricks...")
    uploader = DatabricksUploader()

    # â”€â”€ Step 3: Extract & Upload each table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    success_count = 0
    error_count = 0
    errors = []

    for i, table_name in enumerate(tables, 1):
        logger.info("-" * 50)
        logger.info(f"ğŸ“¦ [{i}/{len(tables)}] Processing table: {table_name}")

        try:
            # Extract from BigQuery
            logger.info(f"   Extracting from BigQuery...")
            df = extractor.extract_table(table_name)
            logger.info(f"   âœ… Extracted {len(df)} rows, {len(df.columns)} columns")

            if len(df) == 0:
                logger.info(f"   â­ï¸  Table is empty, skipping upload.")
                success_count += 1
                continue

            # Sanitize column names for Delta Lake
            import re
            df.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', col).strip('_') for col in df.columns]
            # Remove duplicate column names
            seen = {}
            new_cols = []
            for col in df.columns:
                if col in seen:
                    seen[col] += 1
                    new_cols.append(f"{col}_{seen[col]}")
                else:
                    seen[col] = 0
                    new_cols.append(col)
            df.columns = new_cols

            # Upload to Databricks via SQL Connector
            logger.info(f"   Uploading to Databricks table: {table_name}...")
            uploader.write_with_sql_connector(df, table_name, mode="overwrite")
            logger.info(f"   âœ… Uploaded successfully!")
            success_count += 1

        except Exception as e:
            logger.error(f"   âŒ Error processing {table_name}: {e}")
            errors.append((table_name, str(e)))
            error_count += 1

    # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elapsed = datetime.now() - start_time
    logger.info("=" * 60)
    logger.info(f"ğŸ PIPELINE COMPLETED in {elapsed.total_seconds():.1f}s")
    logger.info(f"   âœ… Success: {success_count}/{len(tables)} tables")
    logger.info(f"   âŒ Errors:  {error_count}/{len(tables)} tables")

    if errors:
        logger.info("\n   Failed tables:")
        for tbl, err in errors:
            logger.info(f"     - {tbl}: {err}")

    logger.info("=" * 60)


if __name__ == "__main__":
    main()
